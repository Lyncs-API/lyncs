{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to dask-mpi\n",
    "\n",
    "Before proceeding to this notebook, we suggest the reading of [\"Introduction to dask\"](Dask.ipynb)\n",
    "\n",
    "## Initialization\n",
    "### Interactive jobs\n",
    "\n",
    "When dask is used interactively (e.g. like here in a notebook), dask-mpi needs to be run in the background as a server with a command of the kind\n",
    "```bash\n",
    "mpirun -n $((N+1)) dask-mpi --no-nanny --scheduler-file scheduler.json --nthreads 1\n",
    "```\n",
    "where `N+1` is the total number of processes having one scheduler and N workers.\n",
    "\n",
    "Then in the notebook we connect to the server by doing\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client(scheduler_file=\"scheduler.json\")\n",
    "```\n",
    "\n",
    "### Batch jobs\n",
    "\n",
    "When dask is used in a script, the script needs to be executed in parallel with a command of the kind\n",
    "```bash\n",
    "mpirun -n $((N+1)) python script.py\n",
    "```\n",
    "and the first line of script.py should be\n",
    "```python\n",
    "from dask_mpi import initialize\n",
    "initialize(nthreads=1, nanny=False)\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "```\n",
    "\n",
    "For more details about dask-mpi refer to its [documentation](https://mpi.dask.org/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "In the following we start start the server and connect to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sh\n",
    "import tempfile\n",
    "\n",
    "# Since dask-mpi produces several file we create a temporary directory\n",
    "tmppath = tempfile.mkdtemp()\n",
    "sh.cd(tmppath)\n",
    "\n",
    "# Here we set the number of workers\n",
    "workers = 8\n",
    "threads_per_worker = 1\n",
    "\n",
    "# The command runs in the background (_bg=True) and the stdout(err) is stored in tmppath+\"/log.out(err)\"\n",
    "server = sh.mpirun(\"-n\", workers+1, \"dask-mpi\", \"--no-nanny\", \"--nthreads\", threads_per_worker,\n",
    "          \"--scheduler-file\", \"scheduler.json\", _bg = True, _out=\"log.out\", _err=\"log.err\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://192.168.10.16:46261</li>\n",
       "  <li><b>Dashboard: </b><a href='http://192.168.10.16:8787/status' target='_blank'>http://192.168.10.16:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>16.68 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://192.168.10.16:46261' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(scheduler_file=tmppath+\"/scheduler.json\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workers\n",
    "\n",
    "Information about the workers can be get using\n",
    "```python\n",
    "client.scheduler_info()[\"workers\"]\n",
    "```\n",
    "that returns a dictionary with keys the workers name and content the last update about the worker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tcp://192.168.10.16:33753',\n",
       " 'tcp://192.168.10.16:34363',\n",
       " 'tcp://192.168.10.16:34605',\n",
       " 'tcp://192.168.10.16:37009',\n",
       " 'tcp://192.168.10.16:39157',\n",
       " 'tcp://192.168.10.16:40921',\n",
       " 'tcp://192.168.10.16:45143',\n",
       " 'tcp://192.168.10.16:45551']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers = list(client.scheduler_info()[\"workers\"].keys())\n",
    "workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Worker',\n",
       " 'id': 8,\n",
       " 'host': '192.168.10.16',\n",
       " 'resources': {},\n",
       " 'local_directory': '/tmp/tmps33b4cfl/worker-e4qqnt8g',\n",
       " 'name': 8,\n",
       " 'nthreads': 1,\n",
       " 'memory_limit': 2085506048,\n",
       " 'last_seen': 1601214775.2609286,\n",
       " 'services': {},\n",
       " 'metrics': {'cpu': 0.0,\n",
       "  'memory': 72876032,\n",
       "  'time': 1601214775.2127116,\n",
       "  'read_bytes': 0.0,\n",
       "  'write_bytes': 0.0,\n",
       "  'num_fds': 37,\n",
       "  'executing': 0,\n",
       "  'in_memory': 0,\n",
       "  'ready': 0,\n",
       "  'in_flight': 0,\n",
       "  'bandwidth': {'total': 100000000, 'workers': {}, 'types': {}}},\n",
       " 'nanny': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The known information are for example\n",
    "client.scheduler_info()[\"workers\"][workers[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed operations\n",
    "We can initialize a group of workers for performing a task using the function \n",
    "```python\n",
    "client.scatter(list, workers = None or workers, broadcast=False, hash=False)\n",
    "```\n",
    "where one of each element of the list will be given to one of the workers in a round-robin based. The list of workers can be selected between the workers available.\n",
    "\n",
    "The content of the list should contain information that the worker needs to proceed.\n",
    "\n",
    "Here a dummy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Future: finished, type: builtins.int, key: int-db0f3161d56946bb8642270285ee7122>,\n",
       " <Future: finished, type: builtins.int, key: int-1929910361ed416ab7c98d664cb6f21d>,\n",
       " <Future: finished, type: builtins.int, key: int-245b2c7b5cdc48de93b62cbfd2e21acd>,\n",
       " <Future: finished, type: builtins.int, key: int-50941fa65e3f484c955683743bd2b1e5>,\n",
       " <Future: finished, type: builtins.int, key: int-ddc03a75227a475ca29e54ca56fb458a>,\n",
       " <Future: finished, type: builtins.int, key: int-14fc35189ce1442fb546530be8e53ca0>,\n",
       " <Future: finished, type: builtins.int, key: int-ef49600552284520a63800e9138055ff>,\n",
       " <Future: finished, type: builtins.int, key: int-a29b2929f8ae4cf5ad2e24e432c2b47a>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = range(len(workers))\n",
    "group = client.scatter(dummy, workers=workers, broadcast=False, hash=False)\n",
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int-ef49600552284520a63800e9138055ff': ('tcp://192.168.10.16:45143',),\n",
       " 'int-db0f3161d56946bb8642270285ee7122': ('tcp://192.168.10.16:33753',),\n",
       " 'int-50941fa65e3f484c955683743bd2b1e5': ('tcp://192.168.10.16:37009',),\n",
       " 'int-245b2c7b5cdc48de93b62cbfd2e21acd': ('tcp://192.168.10.16:34605',),\n",
       " 'int-14fc35189ce1442fb546530be8e53ca0': ('tcp://192.168.10.16:40921',),\n",
       " 'int-ddc03a75227a475ca29e54ca56fb458a': ('tcp://192.168.10.16:39157',),\n",
       " 'int-1929910361ed416ab7c98d664cb6f21d': ('tcp://192.168.10.16:34363',),\n",
       " 'int-a29b2929f8ae4cf5ad2e24e432c2b47a': ('tcp://192.168.10.16:45551',)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.who_has(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[g.result() for g in group]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that they are actually distributed we get the rank of each process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 1, 2, 3, 7, 5, 6, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rank(*args,comm=None):\n",
    "    if comm is None:\n",
    "        from mpi4py.MPI import COMM_WORLD as comm\n",
    "    return comm.rank\n",
    "\n",
    "ranks = client.map(get_rank, group)\n",
    "ranks = [rank.result() for rank in ranks]\n",
    "ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that `rank = 0` is not in the list because indeed the scheduler is running on it and not a worker.\n",
    "\n",
    "Thus any MPI operation need to be run on a communcator involing only the workers and not the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Future: pending, key: create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-0>,\n",
       " <Future: pending, key: create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-1>,\n",
       " <Future: pending, key: create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-2>,\n",
       " <Future: pending, key: create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-3>,\n",
       " <Future: pending, key: create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-4>,\n",
       " <Future: pending, key: create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-5>,\n",
       " <Future: pending, key: create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-6>,\n",
       " <Future: pending, key: create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-7>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_comm(*args, ranks=None, comm=None):\n",
    "    assert ranks\n",
    "    if comm is None:\n",
    "        from mpi4py.MPI import COMM_WORLD as comm\n",
    "    return comm.Create_group(comm.group.Incl(ranks))\n",
    "\n",
    "comms = client.map(create_comm, group, workers=workers, ranks=ranks, actor=True)\n",
    "comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Actor: Intracomm, key=create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-0>,\n",
       " <Actor: Intracomm, key=create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-1>,\n",
       " <Actor: Intracomm, key=create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-2>,\n",
       " <Actor: Intracomm, key=create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-3>,\n",
       " <Actor: Intracomm, key=create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-4>,\n",
       " <Actor: Intracomm, key=create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-5>,\n",
       " <Actor: Intracomm, key=create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-6>,\n",
       " <Actor: Intracomm, key=create_comm-899b7920-c821-45b7-b7bb-4278c163ccf7-7>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comms = [comm.result() for comm in comms]\n",
    "comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object PooledRPCCall.__getattr__.<locals>.send_recv_from_rpc at 0x7fb381684ef0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.scheduler.workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[comm.rank for comm in comms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 8, 8, 8, 8, 8, 8, 8]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reductions = [comm.allreduce(1) for comm in comms]\n",
    "[r.result() for r in reductions]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
