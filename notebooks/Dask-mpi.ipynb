{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to dask-mpi\n",
    "\n",
    "Before proceeding to this notebook, we suggest the reading of [\"Introduction to dask\"](Dask.ipynb)\n",
    "\n",
    "## Initialization\n",
    "### Interactive jobs\n",
    "\n",
    "When dask is used interactively (e.g. like here in a notebook), dask-mpi needs to be run in the background as a server with a command of the kind\n",
    "```bash\n",
    "mpirun -n $((N+1)) dask-mpi --no-nanny --scheduler-file scheduler.json --nthreads 1\n",
    "```\n",
    "where `N+1` is the total number of processes having one scheduler and N workers.\n",
    "\n",
    "Then in the notebook we connect to the server by doing\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client(scheduler_file=\"scheduler.json\")\n",
    "```\n",
    "\n",
    "### Batch jobs\n",
    "\n",
    "When dask is used in a script, the script needs to be executed in parallel with a command of the kind\n",
    "```bash\n",
    "mpirun -n $((N+1)) python script.py\n",
    "```\n",
    "and the first line of script.py should be\n",
    "```python\n",
    "from dask_mpi import initialize\n",
    "initialize(nthreads=1, nanny=False)\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "```\n",
    "\n",
    "For more details about dask-mpi refer to its [documentation](https://mpi.dask.org/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "In the following we start start the server and connect to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sh\n",
    "import tempfile\n",
    "\n",
    "# Since dask-mpi produces several file we create a temporary directory\n",
    "tmppath = tempfile.mkdtemp()\n",
    "sh.cd(tmppath)\n",
    "\n",
    "# Here we set the number of workers\n",
    "workers = 8\n",
    "threads_per_worker = 1\n",
    "\n",
    "# The command runs in the background (_bg=True) and the stdout(err) is stored in tmppath+\"/log.out(err)\"\n",
    "server = sh.mpirun(\"-n\", workers+1, \"dask-mpi\", \"--no-nanny\", \"--nthreads\", threads_per_worker,\n",
    "          \"--scheduler-file\", \"scheduler.json\", _bg = True, _out=\"log.out\", _err=\"log.err\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(scheduler_file=tmppath+\"/scheduler.json\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workers\n",
    "\n",
    "Information about the workers can be get using\n",
    "```python\n",
    "client.scheduler_info()[\"workers\"]\n",
    "```\n",
    "that returns a dictionary with keys the workers name and content the last update about the worker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = list(client.scheduler_info()[\"workers\"].keys())\n",
    "workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The known information are for example\n",
    "client.scheduler_info()[\"workers\"][workers[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed operations\n",
    "We can initialize a group of workers for performing a task using the function \n",
    "```python\n",
    "client.scatter(list, workers = None or workers, broadcast=False, hash=False)\n",
    "```\n",
    "where one of each element of the list will be given to one of the workers in a round-robin based. The list of workers can be selected between the workers available.\n",
    "\n",
    "The content of the list should contain information that the worker needs to proceed.\n",
    "\n",
    "Here a dummy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = range(len(workers))\n",
    "group = client.scatter(dummy, workers=workers, broadcast=False, hash=False)\n",
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.who_has(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[g.result() for g in group]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that they are actually distributed we get the rank of each process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(*args,comm=None):\n",
    "    if comm is None:\n",
    "        from mpi4py.MPI import COMM_WORLD as comm\n",
    "    return comm.rank\n",
    "\n",
    "ranks = client.map(get_rank, group)\n",
    "ranks = [rank.result() for rank in ranks]\n",
    "ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that `rank = 0` is not in the list because indeed the scheduler is running on it and not a worker.\n",
    "\n",
    "Thus any MPI operation need to be run on a communcator involing only the workers and not the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comm(*args, ranks=None, comm=None):\n",
    "    assert ranks\n",
    "    if comm is None:\n",
    "        from mpi4py.MPI import COMM_WORLD as comm\n",
    "    return comm.Create_group(comm.group.Incl(ranks))\n",
    "\n",
    "comms = client.map(create_comm, group, workers=workers, ranks=ranks, actor=True)\n",
    "comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms = [comm.result() for comm in comms]\n",
    "comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.scheduler.workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[comm.rank for comm in comms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reductions = [comm.allreduce(1) for comm in comms]\n",
    "[r.result() for r in reductions]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
